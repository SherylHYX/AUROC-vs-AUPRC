{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d78e836a9446bfca189e3f47ded09690d744047"},"cell_type":"markdown","source":"# Part 1. Read data and class distribution.\n\nIn this part I will read the data and obtain the dataset: Diabetes.\nAfter that, I will analyze a bit the data and see the distribution. The most important question of this part are:\n* How is the data distributed? \n*  Is it balanced?\n\nIs important to know the answer to those questions, to know the evaluation metrics that we wil use."},{"metadata":{"trusted":true,"_uuid":"8c78829efd6de07669bc1b6b28c36258d0e57a98"},"cell_type":"code","source":"diabetes= pd.read_csv(\"../input/diabetes.csv\")\nprint (diabetes.shape)\nprint (\"--\"*30)\nprint (diabetes.info())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4425404351f829eaa8ed1f042e9375672efe2b30"},"cell_type":"markdown","source":"From 768 samples, we have no null values. So the data is perfectly clean. At least in apperance."},{"metadata":{"trusted":true,"_uuid":"7f22381a66ea488bbf1eb7004e2149f4eb3fa1d6"},"cell_type":"code","source":"diabetes.head()\ndiabetes.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8afe970d56407dda20fb088d1f930e5c5e9ee583"},"cell_type":"markdown","source":"Now, we can see the problem. I am not a doctor but I imagine that having a Blood Pressure of 0 is equivalent to be dead. Also, a SkinThickness, insulin, glucose or BMI of 0 do not correlate good with the fact of being a living being.\n\nBecause of that, the data is NOT clean, and we need to take action."},{"metadata":{"_uuid":"b8fdeb1e8d85653db34b74acd73130df0559df4c"},"cell_type":"markdown","source":"But before we take action, it is useful to know how the data is distributed. At the end of the day, I need to do a binary classification. \n\nGiven a few features, I need to predict if someone has diabetes. Yes or no. So, knowing the distribution of the data is essential."},{"metadata":{"trusted":true,"_uuid":"b9f136b5349fea3d0dd149484b88f8a42088670e"},"cell_type":"code","source":"# class distribution\nprint(\" Outcome distribution\")\nprint(diabetes.groupby('Outcome').size())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3ce2cfce91f33bc72a6446d9f94bf618d9c4573"},"cell_type":"markdown","source":"So, we have 500 person with diabetes and 268 whithout it. We can clearly see that is an unbalanced dataset.\nIt is critical to know this for a few reasons:\n\n* The algorithms of machine learning like LogisticRegression os SVM, tend to think that the data is balanced. But if the data is unbalanced they will put more weight on the majority class. In this case, the positive Outcome of diabetes. And this is very very bad.\n* The accuracy is not a useful metric now. For example, I have 100 emails, 99 good emails and 1 spam email. If I created and algorithm that always is going to put all emails in your inbox, then I will reach and accuracy of 99%. But I will not be performing a good job, becauase my job is to be sure that non spam came to your inbox. Because of that I will use other metrics, such as Roc_Auc, sensitivity, specificity...\n"},{"metadata":{"_uuid":"3ce018c7c1bbb5cdab818269516c69f296eba294"},"cell_type":"markdown","source":"# Part 2: EDA.\n\nLets see how the data is distributed and how the features and outcome are correlated."},{"metadata":{"trusted":true,"_uuid":"f919a8ce2921163b2754c9e024aa8869fdd13760"},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d25a995d9987e69e324eb04f564319fad1e6882c"},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{"_uuid":"6a727841ec1768d7e78be27822b9ee4ae9f39180"},"cell_type":"markdown","source":"If we look at the data, we see that some features has null values, in this case with the apperances of 0. Because later I will perform some modeling with the raw data, now I am going to clean the data in a copy dataset."},{"metadata":{"trusted":true,"_uuid":"cbf582eb6d5c3275c2b76f3331b39504cde705fc"},"cell_type":"code","source":"#1) Replace 0 values to NaN values. Then sum the null values in each of those features,\n#to know how many null values we have.\ndiabetes_copy=diabetes.copy(deep=True) ## We will need later the diabetes dataset with the 0s.\n\ndiabetes[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]]=diabetes[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]].replace(0,np.NaN)\nprint (diabetes.isnull().sum())\ndiabetes.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2585b6159b01b88abeb6b5f54f09bb778091f26a"},"cell_type":"markdown","source":"I will replace all those values with the median.  The SkinThickness and insulin have a lot of null values. Because of that I will need to pay extra attention to those features in the visualizations tha I will make later."},{"metadata":{"trusted":true,"_uuid":"ec326b93ce55d32db4d252f4ea95705b0e827a1b"},"cell_type":"code","source":"# We replace the NaN values with the mean or median.\n# Glucose and BloodPressure dont have much outliers, and we need little data to fill. The mean will be enough.\n# The others, has a huge disparity between some samples, and we need a lot of data. So the median is best.\ndiabetes[\"Glucose\"].fillna(diabetes[\"Glucose\"].mean(),inplace=True)\ndiabetes[\"BloodPressure\"].fillna(diabetes[\"BloodPressure\"].mean(),inplace=True)\ndiabetes[\"SkinThickness\"].fillna(diabetes[\"SkinThickness\"].median(),inplace=True)\ndiabetes[\"Insulin\"].fillna(diabetes[\"Insulin\"].median(),inplace=True)\ndiabetes[\"BMI\"].fillna(diabetes[\"BMI\"].median(),inplace=True)\n\nprint (diabetes.isnull().sum())\nprint ('--'*40)\ndiabetes.info()\nprint ('--'*40)\ndiabetes.head()\ndiabetes.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffce22b33040ac6f70986f9ea64156781d894b64"},"cell_type":"markdown","source":"Perfect. The data is now perfectly clean."},{"metadata":{"_uuid":"aaa832d60e8da476db15118d0543c4c5f168c01d"},"cell_type":"markdown","source":"## Visualizations"},{"metadata":{"trusted":true,"_uuid":"8a965502fe5a37dadef44f9effb1c93c8750bd16"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(diabetes['Pregnancies'],kde=False,bins=50)\nplt.title('Pregnancies per Person on Pima People')\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f075756cea1e4c0c6bf2d4d903c27dff740cb6d"},"cell_type":"code","source":"print('Average amount of children had by a Pima woman: ' + str(diabetes['Pregnancies'].mean()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0d330986b60f237595b0f7ff99c5824f4ab15d4"},"cell_type":"markdown","source":"Some of the Pima women are having a lot of children, but the most popular case is 1 child per women. In anycase, later, with the bivariate plots we will see how is the relationship between pregnancies and diabetes."},{"metadata":{"trusted":true,"_uuid":"d0d689e705f12e79a94074990e3c7fe7c015d315"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(diabetes['Glucose'],kde=False,bins=50)\nplt.title('Glucose per Person on Pima People')\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c0a3cc11cbe58fd9141aa7fa5b1c64f5fdb7e46"},"cell_type":"markdown","source":"Kind of a normal distribution. This is good. I will not need to make any changes on this variable. Later, we will see how is the correlation between Glucose and Diabetes.\n"},{"metadata":{"trusted":true,"_uuid":"78e0ea0a0ae4c0a1f4995a57fb69f37abd9a4bf3"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(diabetes['BloodPressure'],kde=False,bins=50)\nplt.title('Blood Pressure of Pima Indian People')\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63c5c3b38ee53d9079bfc0a5c93e234dc5709f9b"},"cell_type":"markdown","source":"Again, good news. It seems to be a normal distribution, and that means no change on the data later. Is a good distribution for the algorithms. Later we will see the relation it has with other features."},{"metadata":{"trusted":true,"_uuid":"6cbb4061ab2a44a5b64d987a945f52c891bfe437"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(diabetes['SkinThickness'],kde=False,bins=50)\nplt.title('Skin Thickness of Pima Indian People')\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51fe1a31771ee926936b705e4e7f2e0477c190af"},"cell_type":"markdown","source":"Is completely normal that it looks like that. Remember that we had 227 null values on this data that were filled by the median. "},{"metadata":{"trusted":true,"_uuid":"e092ebf8bbaf92ef15f213fe4c8e8676e6b39f63"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(diabetes['Insulin'],kde=False,bins=50)\nplt.title('Insulin of Pima Indian People')\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c0a707359541c525a6d992601558a9ca41cd508"},"cell_type":"markdown","source":"Same case scenario as before. We will see later the correlation between features."},{"metadata":{"trusted":true,"_uuid":"634b357532fec66cb4c7f9481e04c60f2d7b3cf0"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(diabetes['BMI'],kde=False,bins=50)\nplt.title('BMI of Pima Indian People')\nplt.ylabel('Number of People')\nplt.show()\nprint('Average BMI of a Pima Person: ' + str(diabetes['BMI'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2115cf94faadd49a8ef63a57805d930d1157b87"},"cell_type":"markdown","source":"Well it looks like a normal distribution, which is good. "},{"metadata":{"trusted":true,"_uuid":"eab60b770ce40436776a1093fcef50e59524bacd"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(diabetes['DiabetesPedigreeFunction'],kde=False,bins=50)\nplt.title('Diabetes Pedigree Function of Pima Indian People')\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de56e2a1bd4a4299f69b772511f899d7cc604ca2"},"cell_type":"markdown","source":"Well it looks like a part of a log function or an inversal exp function. I do not like that, and later I will try to modify the data to make a normal gaussian distribution of this data. The reason as I have mentioned before is to make my algorithm work better. [Some of them]"},{"metadata":{"trusted":true,"_uuid":"7b3c95abf5ada301de3c4f746d6a7823ea2f8ce0"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(diabetes['Age'],kde=False,bins=50)\nplt.title('Age of Pima Indian People')\nplt.ylabel('Number of People')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0480b592e5be077aa5209233cc74cbddfd353669"},"cell_type":"markdown","source":"Same case as before. It has a lot of young women, and the data is not normalli distributed. Later, I will fix that. It has a lot of young women, so if Age is an important feature, then that majority of young women will make the predictions fall a bit. At the end of the day, they have a lot of weight on this feature."},{"metadata":{"trusted":true,"_uuid":"cda41bf398bfb4e90ad16362cfbe16900c1f7915"},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nsns.countplot(x='Outcome',data=diabetes)\nplt.title('Positive Outcome to Diabetes in Dataset')\nplt.ylabel('Number of People')\nplt.show()\nprint('Ratio of Population with Diabetes: ' + str(len(diabetes[diabetes['Outcome']==1])/len(diabetes)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16728b0d9ba4af3966202cff3974adadf20911ba"},"cell_type":"markdown","source":"So we have an inbalanced dataset. Is not huge, but I cannot allow to exist. I need to mitigate its effect. Because if that, later I will use SMOTE oversampling.\n"},{"metadata":{"_uuid":"b49f3bc52c2ad3c620daf690f78d63ab065c2291"},"cell_type":"markdown","source":"### Bivariate Analysis"},{"metadata":{"trusted":true,"_uuid":"f978097a37c0e96cfc06fe8daa162bfa0078b818"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.heatmap(diabetes.corr(),cmap='YlGn',annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a18e1a7056721a02f7f1f0161b745307b27c3bbf"},"cell_type":"markdown","source":"* There are not strong correlation between features which is good. This way, I will not need to drop any feature because of a problem of overfitting.\n* There is no feature with a very strong correlation with the label, Outcome. The best is Glucose. I think, that if we had had more Insulin data, it would have a strong correlation. But with what we have, the better predictor is glucose.\n* There are some obvious correlations:\n    * Skin Thickness with BMI. The fatter you are, the more Skincircunference in your arms.\n    * Age with pregnancies. The older you are, it is more possible to have a child."},{"metadata":{"trusted":true,"_uuid":"13ed9c918aa4aeb71c9f8f2275a6bdb90627b545"},"cell_type":"code","source":"g = sns.FacetGrid(diabetes, col=\"Outcome\",size=5)\ng = g.map(plt.hist, \"Glucose\",bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8579aabd772c2738b9c8c131c6e7b95a9e562e25"},"cell_type":"code","source":"print('Average number of glucose for positive outcomes: ' + str(diabetes[diabetes['Outcome']==1]['Glucose'].mean()))\nprint('Average number of glucose for negative outcomes: ' + str(diabetes[diabetes['Outcome']==0]['Glucose'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40f731c87ee965bc088f765092401c3c119735cf"},"cell_type":"markdown","source":"There is a normal looking population for Pima women without diabetes and their glucose levels. The population with diabetes have a different story to tell. The population with diabetes looks to have higher glucose levels."},{"metadata":{"trusted":true,"_uuid":"4e5dedf916c153a7347e875f366e08ed0211aa2b"},"cell_type":"code","source":"g = sns.FacetGrid(diabetes, col=\"Outcome\",size=5)\ng = g.map(plt.hist, \"BMI\",bins=30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ffe6517257c04f8bab734e3267509f3c33fae4"},"cell_type":"markdown","source":"Both BMI readings for people with and without diabetes seem to have normal distributions. BMI of people with diabetes looks to be with a bit of  higher values, but not something exagerated. But yes, the more BMI the more possible to have diabetes."},{"metadata":{"trusted":true,"_uuid":"dc8e62cd84cb77f26718d12ac5603bdd0c4321ac"},"cell_type":"code","source":"print('Average Body Mass Index of a Pima woman without diabetes: ' + str(diabetes[diabetes['Outcome']==0]['BMI'].mean()))\nprint('Average Body Mass Index of a Pima woman with diabetes: ' + str(diabetes[diabetes['Outcome']==1]['BMI'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec731ef37afb2729d4af60f31efca0eb17c2e8ba"},"cell_type":"markdown","source":"There is no doubt that the population of Pima women are overweight. However, the population with diabetes is even more so."},{"metadata":{"trusted":true,"_uuid":"950ab3eb8abcf2c79bcd927c415df987cb73dcb9"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(data=diabetes,x='Outcome',y='Pregnancies')\nplt.title('Pregnancies Among Diabetes Outcomes.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3dccd63c0c0dacb56a25a067b115540c78d7ea5"},"cell_type":"markdown","source":"So the women with fewer childs had less diabetes. I think that there is not much relation between Pregnancies and Outcome, but less think a bit.\n* If you have a lot of children, you need to fed them.  And in those cases the less expensive food are carbohhydrates which is the food who moves the Glucose-Insulin cycle. So, perharps is for that. But I dont know.\n* Lets see this other graph to grasp a bit of more domain knowledge."},{"metadata":{"trusted":true,"_uuid":"138e7403f8388ad56bd7c5e4e5bbbd48c4339901"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(x='Pregnancies',data=diabetes,hue='Outcome')\nplt.title('Diabetes Outcome to Pregnancies')\nplt.show()\nprint('Average number of pregnancies for positive outcomes: ' + str(diabetes[diabetes['Outcome']==1]['Pregnancies'].mean()))\nprint('Average number of pregnancies for negative outcomes: ' + str(diabetes[diabetes['Outcome']==0]['Pregnancies'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"128025f58417dd0b3e762495ea38dc862c92efc8"},"cell_type":"code","source":"plt.figure(figsize=(13,6))\nsns.countplot(x='Age',data=diabetes,hue='Outcome')\nplt.title('Diabetes Outcome to Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61052ab636dfebaebdaf24117215b495744d63d6"},"cell_type":"code","source":"print('Average number of age for positive outcomes: ' + str(diabetes[diabetes['Outcome']==1]['Age'].mean()))\nprint('Average number of age for negative outcomes: ' + str(diabetes[diabetes['Outcome']==0]['Age'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42d91e1d4db11b01366e47e9b7e4c4427a7ddfce"},"cell_type":"markdown","source":"So, the older you are the most opportunities to have diabetes. And it is normal, at the end of the day, the older you are the more possibilites of having destroy your body with bad habits."},{"metadata":{"trusted":true,"_uuid":"61ab84c1f1c82789e02463281d08ed68cf94ca6d"},"cell_type":"code","source":"plt.figure(figsize=(13,6))\nsns.countplot(x='SkinThickness',data=diabetes,hue='Outcome')\nplt.title('Diabetes Outcome to SkinThickness')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3194a549fcbcdbb2eb02a94ea4b35b26e6b7aab1"},"cell_type":"code","source":"print('Average number of skin thickness for positive outcomes: ' + str(diabetes[diabetes['Outcome']==1]['SkinThickness'].mean()))\nprint('Average number of skin thickness for negative outcomes: ' + str(diabetes[diabetes['Outcome']==0]['SkinThickness'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbb3fdf5d16c427361e44268ab2fdf4bbdb5d143"},"cell_type":"markdown","source":"Well this data is no so representative. Remember that we plugged in more than 250 values that did not exist. Anyways, what is clear is that the more fat you have around your arm [that is the measure of skin thickness], the more probabilities of developing diabetes. So, it has a lot of relation with BMI."},{"metadata":{"trusted":true,"_uuid":"868c59a3f001b78deb995d532a4b5df50199dc4a"},"cell_type":"code","source":"# Diabetes has a lot of values, so to plot it I need to make a few changes. The most important one is to\n# divide it in quartiles.\ndiabetes_copy2= diabetes.copy(deep=True)\ndiabetes_copy2[\"InsulinBins\"]=pd.qcut(diabetes[\"Insulin\"],4)\n#Now we can plot\nplt.figure(figsize=(13,6))\nsns.countplot(x='InsulinBins',data=diabetes_copy2,hue='Outcome')\nplt.title('Diabetes Outcome to Insulin')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b66659189ab71a567c29d15e3ada242059562be4"},"cell_type":"code","source":"print('Average number of Insulin for positive outcomes: ' + str(diabetes[diabetes['Outcome']==1]['Insulin'].mean()))\nprint('Average number of Insulin for negative outcomes: ' + str(diabetes[diabetes['Outcome']==0]['Insulin'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bad58f114d5561f1b6bbcf6ac587d0a9dfbc34c"},"cell_type":"markdown","source":"Well same case as skin thickness. I think that with new data, Insulin will be essential to predict diabetes. Anyways, is clear that the higher blood insulin, the higher possibilites of developing diabetes. It has correlation with Glucose."},{"metadata":{"trusted":true,"_uuid":"4a21967262495df0d4bc4ef53e4a30a9f2f2b787"},"cell_type":"code","source":"diabetes_copy2[\"DiabetesPedigreeBins\"]=pd.qcut(diabetes[\"DiabetesPedigreeFunction\"],4)\n# Same reason as in insulin\nplt.figure(figsize=(13,6))\nsns.countplot(x='DiabetesPedigreeBins',data=diabetes_copy2,hue='Outcome')\nplt.title('Diabetes Outcome to DiabetesPedigreeFunction')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb95d512222d14a7b099e505e3ae42001d079c71"},"cell_type":"code","source":"print('Average number of Diabetes Pedigree Function for positive outcomes: ' + str(diabetes[diabetes['Outcome']==1]['DiabetesPedigreeFunction'].mean()))\nprint('Average number of Diabetes Pedigree Function for negative outcomes: ' + str(diabetes[diabetes['Outcome']==0]['DiabetesPedigreeFunction'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5c31083cfc6581e3a72c4ebc6f68104b953efe4"},"cell_type":"markdown","source":"This one is easy and not so easy. The data tell us that if your family has a tendency of having diabetes, you have more possibilites. And I agree with that, but the data does not tell us if this is a genetic question.\n\n* At the end of the day, our nuclear family show us with their actions what is good and bad when we are children. If my parents have developed diabetes because of their bad habits, I will adopt those habits and develope diabetes.\n* It also can be a genetic problem. An endocrine problem pass though generations.\n* In any case, the data do not specify. So, let us see us."},{"metadata":{"trusted":true,"_uuid":"d9b35fb66797952c94247c0b7d86e6ba2545b82a"},"cell_type":"code","source":"diabetes_copy2[\"GlucoseBins\"]=pd.qcut(diabetes[\"Glucose\"],4)\nplt.figure(figsize=(13,6))\nsns.countplot(x='GlucoseBins',data=diabetes_copy2,hue='DiabetesPedigreeBins')\nplt.title('Glucose and Diabetes Pedigree relationship.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e3cf6b3748fa2eafb45137c564444af8893ac62"},"cell_type":"markdown","source":"* The people with less Glucose, tend to be the ones with less family with diabetes. And glucose levels have relation with food, not hormones, so it is not an endocrine problem. With Insulin yes, with glucose no.\n* The people with more glucose, tend to have more family with diabetes.\n    * So, it seems that it is more a habit problem than a genetic problem.\n    * But we are not sure. We need more data, at least insulin data to come to understand well the problem. Is for that reason that i am not going to drop the Pedigree Function. \n    * If the Pedigree function is just for bad habits, then it will not help to predict anything in the real world. Well, it will help to predict how bad habits pass from one generation to the next, but it will not help to say: You are going to have diabetes."},{"metadata":{"trusted":true,"_uuid":"0f1303a039face56d32c61e26757ececa48e2512"},"cell_type":"code","source":"plt.figure(figsize=(13,6))\nsns.countplot(x='BloodPressure',data=diabetes,hue='Outcome')\nplt.title('Diabetes Outcome to BloodPressure')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4faef836d32b0391017d0ca06e09ff13b3ef72c7"},"cell_type":"code","source":"print('Average number of Blood Presure for positive outcomes: ' + str(diabetes[diabetes['Outcome']==1]['BloodPressure'].mean()))\nprint('Average number of Blood Pressure for negative outcomes: ' + str(diabetes[diabetes['Outcome']==0]['BloodPressure'].mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffc0de1a4e611a60f756d4851b80b2d6214a838b"},"cell_type":"markdown","source":"Well, higher blood pressure, more possibilites of diabetes. But it does not seem to be particularly important. Also, I think that it have relation with BMI. The fatter you are, the more blood pressure in a relaxed state."},{"metadata":{"trusted":true,"_uuid":"8f778d89b02c6c4a9174bb9ff694c85854676027"},"cell_type":"code","source":"diabetes_copy2[\"BloodPressureBins\"]=pd.qcut(diabetes[\"BloodPressure\"],4)\ndiabetes_copy2[\"BMIBins\"]=pd.qcut(diabetes[\"BMI\"],4)\n\nplt.figure(figsize=(13,6))\nsns.countplot(x='BloodPressureBins',data=diabetes_copy2,hue='BMIBins')\nplt.title('BMI relation with BloodPressure')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37f2514f40bea7217860274b5b0098e0f71e772f"},"cell_type":"markdown","source":"And the relation is clear as water. When the blood pressure is low, the BMI is low. When the Blood Pressure is high, BMI is high."},{"metadata":{"_uuid":"56e423b8c40d7bd30049f49b7d531fc9370a4402"},"cell_type":"markdown","source":"### Features and relationships.\n\n* BMI, skin_thickness and blood pressure I think that have a close relation.\n* Age and pregnancies.\n* Glucose, Insulin and Pedigree.\n* Because of all of this, if the dataset was huge, I will drop later pregnancies, blood pressure and other data that is not so important. But this dataset is small, so there is no problem. I will keep all the features."},{"metadata":{"_uuid":"3e8dbb0e247ebd314df20fdfe3ce26923f5da53c"},"cell_type":"markdown","source":"## Distributed data in a single plot\n\nHow is the data distributed among the features? It is useful to know, becauase that way we can do a better feature scaling latter."},{"metadata":{"trusted":true,"_uuid":"1eb297e1d983cd47613f9a883983728499b90dce"},"cell_type":"code","source":"diabetes.hist(figsize=(8,8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"215e3d495bd4fbc0bb57b4a6efdcfb46a685b2a7"},"cell_type":"markdown","source":"Well, I will do Standar Scaling, to normalize the data. "},{"metadata":{"trusted":true,"_uuid":"3e404c9133a7950e904a0f5df4c30b9c1674be27"},"cell_type":"markdown","source":"# Part 3: Modeling and feature_engineering"},{"metadata":{"_uuid":"b63ccdd38b8173321069273da225ae33316a2fb7"},"cell_type":"markdown","source":"Because of that unbalanced dataset I will use roc_auc score as the metric of success. Also I will use the confusion matrix to calculate the sensitivity and specificity. Lasty, I will also use accuracy, but this one does not prove anything in an unbalanced dataset.\n\nI will use a bunch of algorithms. The ones that perform better, will be analyze in more detail [sensitivity, specifity and roc_auc score]"},{"metadata":{"_uuid":"9dc0d7ff0729aec2d62db2b50fa0c5df712afdf9"},"cell_type":"markdown","source":"# Baseline model.\n\nI will calculate the roc_auc in each step of cleaning, scaling and oversampling the data, to see how changes the roc_auc, sensitivity, specifity and accuracy through each step.\n\nFirst thing first. The data with no changes."},{"metadata":{"_uuid":"35066b08bc4b8e39b7486336f1ad96ca99a26429"},"cell_type":"markdown","source":"### All Algorithms baseline model"},{"metadata":{"trusted":true,"_uuid":"c843c8c897bac860af137de576046a4234247336"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom imblearn.over_sampling import SMOTE  \nfrom imblearn.pipeline import Pipeline as Pipeline\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import (GridSearchCV,StratifiedKFold)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bfcc5b480c7fdb5ac3542d68e8e6f9737352cef"},"cell_type":"markdown","source":"Lets remember that this first modeling is with the data without been clean. The original data."},{"metadata":{"trusted":true,"_uuid":"ab9218128de71b7f3a64ee585341f9075e2540d5"},"cell_type":"code","source":"X=diabetes_copy.drop([\"Outcome\"], axis=1)\ny=diabetes_copy[\"Outcome\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0df9078e2ece547b67b4844c6c47ec4fa5a022c"},"cell_type":"code","source":"scoring = 'roc_auc'\nseed=7\nmodels = [] # Here I will append all the algorithms that I will use. Each one will run in all the created datasets.\nmodels.append(('LR', LogisticRegression())) \nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('AdaBoost', AdaBoostClassifier()))\n\nprint(\"evaluation metric: \" + scoring)    \nresults=[]\nnames=[]\nfor name, model in models:\n        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n        cv_results = model_selection.cross_val_score(model,X, y, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        \n        \n        print (\"Algorithm :\",name)\n        print (\" Baseline CV mean: \", cv_results.mean())\n        print (\"--\"*30)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca1b30e678bc9d7ba409e32fe54a979da3f9d36e"},"cell_type":"markdown","source":"So, the best algorithms are Logistic Regression and Linear Discriminant."},{"metadata":{"_uuid":"ae4a20a29cbdf1b50e2244c5cb53008047572e80"},"cell_type":"markdown","source":"### Logistic Regression Baseline model"},{"metadata":{"trusted":true,"_uuid":"a8ecd99c8321c11041d0b3f9f6f2c9bf874c7f39"},"cell_type":"code","source":"train_X,test_X,train_y,test_y = train_test_split (X,y,test_size=0.2,random_state=3)\n\nmodel1= LogisticRegression()\nfit1 =model1.fit(train_X,train_y)\nprediction1= model1.predict(test_X)\nconfusion= metrics.confusion_matrix(test_y, prediction1)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\nprint (\"Baseline model accuracy: \", metrics.accuracy_score(test_y,prediction1))\nprint (\"--\"*30)\nprint (\"Baseline matrix confusion: \", \"\\n\",metrics.confusion_matrix(test_y,prediction1))\nprint (\"--\"*30)\nprint (\"Baseline sensitivity: \", TP / float(FN + TP))\nprint (\"--\"*30)\nprint (\"Baseline model specificity: \", TN / (TN + FP))\nprint (\"--\"*30)\nprint (\"Baseline roc auc score: \", \"\\n\", metrics.roc_auc_score(test_y,prediction1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f1bcee9aa315887f25c378685db0d6854b8c087"},"cell_type":"markdown","source":"So it has a good ROC AUC score. But, it fails tremendosly in the sensitivity. And the sensitivity, the recall is the thing that we want to improve. \n*Sensitivity: When the actual value is positive, how often is the prediction correct?\n\nHow \"sensitive\" is the classifier to detecting positive instances? Or in other words. If you have diabetes, will it detect it? The sensitivity is going to measure that. So, it needs to increase... A lot.\n\nThe problem with sensitivity is that it will detect diabetes to people who dont have diabetes. But it is better that, than sending someone home telling them that they do not have diabetes.\n\n* For example, Imagine if you have cancer. The sensitivity is never going to fail if someone has cancer. But it can tell that someone has cancer withour having it.\n* The specificity is going to tell us that the person who have a positive test is going to have cancer for sure. BUT, and it is a big but, if someone who have cancer, have a negative test is going to pass undetected.\n* So, for differente imbalanced datasets, you are going to choose between sensitivity and specificity. In all the medical datasets, choose sensitivity."},{"metadata":{"_uuid":"d5bf7baa6d50188e38157548643e9249bac0efd6"},"cell_type":"markdown","source":" ### Linear discriminant analysis Classifier Baseline Model"},{"metadata":{"trusted":true,"_uuid":"cea816732360c9f9e17d9ff0fbad51762ad94fae"},"cell_type":"code","source":"X=diabetes_copy.drop([\"Outcome\"], axis=1)\ny=diabetes_copy[\"Outcome\"]\n\ntrain_X,test_X,train_y,test_y = train_test_split (X,y,test_size=0.2,random_state=3)\n\nmodel2= LinearDiscriminantAnalysis()\nfit2 =model2.fit(train_X,train_y)\nprediction2= model2.predict(test_X)\nconfusion= metrics.confusion_matrix(test_y, prediction2)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\nprint (\"Baseline model accuracy: \", metrics.accuracy_score(test_y,prediction2))\nprint (\"--\"*30)\nprint (\"Baseline matrix confusion: \", \"\\n\",metrics.confusion_matrix(test_y,prediction2))\nprint (\"--\"*30)\nprint (\"Baseline sensitivity: \", TP / float(FN + TP))\nprint (\"--\"*30)\nprint (\"Baseline model specificity: \", TN / (TN + FP))\nprint (\"--\"*30)\nprint (\"Baseline roc auc score: \", \"\\n\", metrics.roc_auc_score(test_y,prediction2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bea1b4f579452e2200595ffcf96bf423a2d8bd7"},"cell_type":"markdown","source":"So in the baseline model, LDA is the best. It performs poorly, but has better sensitivity than Logistic Regression."},{"metadata":{"_uuid":"cf9d35d3ee723c0fe262d27a0a87c7b7a33bba44"},"cell_type":"markdown","source":"#  Model with the data clean.\n\nThe step that  I have perfomed at the beginning"},{"metadata":{"_uuid":"18878473c55016ea0dcddbd7e74130811cec6d17"},"cell_type":"markdown","source":"### All algorithms model with the data clean"},{"metadata":{"trusted":true,"_uuid":"ad19c7f7415c62404d865983672090e7016feec3"},"cell_type":"code","source":"X=diabetes.drop([\"Outcome\"], axis=1)\ny=diabetes[\"Outcome\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"966b0406ffc6047b89c1cdbd3e7b25a2f980b218"},"cell_type":"code","source":"scoring = 'roc_auc'\nseed=7\nmodels = [] # Here I will append all the algorithms that I will use. Each one will run in all the created datasets.\nmodels.append(('LR', LogisticRegression())) \nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('AdaBoost', AdaBoostClassifier()))\n\nprint(\"evaluation metric: \" + scoring)    \nresults=[]\nnames=[]\nfor name, model in models:\n        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n        cv_results = model_selection.cross_val_score(model,X, y, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        \n        \n        print (\"Algorithm :\",name)\n        print (\" Data clean CV mean: \", cv_results.mean())\n        print (\"--\"*30)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ffd87014d36d1e2992119dc479f226d6c929a6e"},"cell_type":"markdown","source":"Again, the best algorithms are LDA and Logistic Regression"},{"metadata":{"_uuid":"fa52b56b90504a59903bb3a368c5d57259ffbf52"},"cell_type":"markdown","source":"## Logistic Regression data clean"},{"metadata":{"trusted":true,"_uuid":"f5adca4a2fd3b9f17a9ffb65f01bfb8b7e2fe09d"},"cell_type":"code","source":"train_X,test_X,train_y,test_y = train_test_split (X,y,test_size=0.2,random_state=3)\n\nmodel3=LogisticRegression()\nfit3 =model3.fit(train_X,train_y)\nprediction3= model3.predict(test_X)\nconfusion= metrics.confusion_matrix(test_y, prediction3)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\nprint (\"Data clean model roc auc score: \",metrics.roc_auc_score(test_y,prediction3))\nprint (\"__\"*30)\nprint (\"Data clean model matrix confusion: \", \"\\n\",metrics.confusion_matrix(test_y,prediction3))\nprint (\"--\"*30)\nprint (\"Data clean model sensitivity: \", TP / float(FN + TP))\nprint (\"--\"*30)\nprint (\"Data clean model specificity: \", TN / (TN + FP))\nprint (\"__\"*30)\nprint (\"Data clean model accuracy: \", metrics.accuracy_score(test_y,prediction3))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e6361e33b453999fa6323b7249b89887b8ec8e3"},"cell_type":"markdown","source":"Well, a disaster. The sensitivity has plummed. It could be because of the fill in the NaN values with the median. Lets see how performs LDA."},{"metadata":{"_uuid":"212616549da5cbba2fd4a92dfe7cd7a82700e1ca"},"cell_type":"markdown","source":"### Linear discriminant analysis with the data clean"},{"metadata":{"trusted":true,"_uuid":"55982962a5529b544d7741c3e270f67e08a4622e"},"cell_type":"code","source":"train_X,test_X,train_y,test_y = train_test_split (X,y,test_size=0.2,random_state=3)\n\nmodel4=LinearDiscriminantAnalysis()\nfit4 =model4.fit(train_X,train_y)\nprediction4= model4.predict(test_X)\nconfusion= metrics.confusion_matrix(test_y, prediction4)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\nprint (\"Data clean model roc auc score: \",metrics.roc_auc_score(test_y,prediction4))\nprint (\"__\"*30)\nprint (\"Data clean model matrix confusion: \", \"\\n\",metrics.confusion_matrix(test_y,prediction4))\nprint (\"--\"*30)\nprint (\"Data clean model sensitivity: \", TP / float(FN + TP))\nprint (\"--\"*30)\nprint (\"Data clean model specificity: \", TN / (TN + FP))\nprint (\"__\"*30)\nprint (\"Data clean model accuracy: \", metrics.accuracy_score(test_y,prediction4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39b84ae4871e5ee0feaf5711324d2a47df1c62ba"},"cell_type":"markdown","source":"Also, the results looks very very bad. Lets scale and normalize the datasets to make everything work better. The reason is for the filling with the median. It has change the distribution, so it is time to normalize it, and with that lets expect an increment in sensitivity"},{"metadata":{"_uuid":"e6ab389b603fb4680777626471994d33ab165ed7"},"cell_type":"markdown","source":" # Modeling with the data clean and scaled"},{"metadata":{"trusted":true,"_uuid":"fe65893b6c1bc42da356b3eb9d512ce0de8312a0"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX=diabetes.drop([\"Outcome\"],axis=1)\nprint (X.info())\ncolumnas=[\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"]\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nX=pd.DataFrame(X_scaled, columns=[columnas])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e12f8022d8026106ef1acd93d904a4454c072113","scrolled":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86a443ac421bcb41100a141d0960a16b4a1c0a79"},"cell_type":"markdown","source":"### All algorithms modeling with the data clean and scaled."},{"metadata":{"trusted":true,"_uuid":"b1afbc51d1d4faa817b0a7b2ac1032e216bb3f78"},"cell_type":"code","source":"scoring = 'roc_auc'\nseed=7\nmodels = [] # Here I will append all the algorithms that I will use. Each one will run in all the created datasets.\nmodels.append(('LR', LogisticRegression())) \nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('AdaBoost', AdaBoostClassifier()))\n\nprint(\"evaluation metric: \" + scoring)    \nresults=[]\nnames=[]\nfor name, model in models:\n        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n        cv_results = model_selection.cross_val_score(model,X, y, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        \n        \n        print (\"Algorithm :\",name)\n        print (\" Data clean & scaled CV mean: \", cv_results.mean())\n        print (\"--\"*30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b349f2743bc34f9c9c544e13191d97baee20b55c"},"cell_type":"markdown","source":"### Logistic Regression data clean and scaled"},{"metadata":{"trusted":true,"_uuid":"9d70145266dc0e1d5a46a3942a376fd8a303666b"},"cell_type":"code","source":"train_X,test_X,train_y,test_y = train_test_split (X,y,test_size=0.2,random_state=3)\n\nmodel5= LogisticRegression()\nfit5 =model5.fit(train_X,train_y)\nprediction5= model5.predict(test_X)\nconfusion= metrics.confusion_matrix(test_y, prediction5)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\nprint (\"Data clean & scaled roc auc score: \", \"\\n\", metrics.roc_auc_score(test_y,prediction5))\nprint (\"--\"*30)\nprint (\"Data clean & scaled matrix confusion: \", \"\\n\",metrics.confusion_matrix(test_y,prediction5))\nprint (\"--\"*30)\nprint (\"Data clean & scaled model sensitivity: \", TP / float(FN + TP))\nprint (\"--\"*30)\nprint (\"Data clean & scaled model specificity: \", TN / (TN + FP))\nprint (\"__\"*30)\nprint (\"Data clean & scaled model accuracy: \", metrics.accuracy_score(test_y,prediction5))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aaad0fd12e23a3dd9e12fb08cdac8cb05afa239"},"cell_type":"markdown","source":"And now we can see an improve."},{"metadata":{"_uuid":"2109de376b8d715970b452f65e585ace63c6a86c"},"cell_type":"markdown","source":"### Linear discriminant Analysis Data clean and scaled"},{"metadata":{"trusted":true,"_uuid":"f513ce24995f36015cbdac922491ec57b52c7e58"},"cell_type":"code","source":"train_X,test_X,train_y,test_y = train_test_split (X,y,test_size=0.2,random_state=3)\n\nmodel= LinearDiscriminantAnalysis()\nfit6 =model.fit(train_X,train_y)\nprediction6= model.predict(test_X)\nconfusion= metrics.confusion_matrix(test_y, prediction6)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\nprint (\"Data clean & scaled roc auc score: \", \"\\n\", metrics.roc_auc_score(test_y,prediction6))\nprint (\"--\"*30)\nprint (\"Data clean & scaled matrix confusion: \", \"\\n\",metrics.confusion_matrix(test_y,prediction6))\nprint (\"--\"*30)\nprint (\"Data clean & scaled model sensitivity: \", TP / float(FN + TP))\nprint (\"--\"*30)\nprint (\"Data clean & scaled model specificity: \", TN / (TN + FP))\nprint (\"__\"*30)\nprint (\"Data clean & scaled model accuracy: \", metrics.accuracy_score(test_y,prediction6))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2507e73e8678a2cc461773200fd2c13229a78628"},"cell_type":"markdown","source":"# Smoke, GridSearch, CV.\n\nBecause Logistic Regression and LDA have been the best algorithms, I will keep working with them. \n* At the beginning I analized the distribution of the outcome. It showed and inbalanced dataset. And the problem with that is that some algorithms, like Logistic Regression, place more weight in the majority class. In this case, not having diabetes--> Low sensitivity---> Something awful.\n* To avoid that, we can oversample or undersample.\n    * Because we have few samples i Will oversample. And I will do it with the SMOTE technique."},{"metadata":{"_uuid":"512b1c8be3dd59b89d06a4508ec388ee1cadcacd"},"cell_type":"markdown","source":"### Logistic Regression Oversampling with Smote"},{"metadata":{"trusted":true,"_uuid":"ba8313f4aeada4d2939707267e8c983d43a88e5a","scrolled":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE  \nfrom imblearn.pipeline import Pipeline as Pipeline\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import (GridSearchCV,StratifiedKFold)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=444, stratify=y)\n\npipe = Pipeline([\n    ('oversample', SMOTE(random_state=444)),\n    ('clf', LogisticRegression(random_state=444, n_jobs=-1))\n    ])\n\nskf = StratifiedKFold(n_splits=10)\nparam_grid = {'clf__C': [0.001,0.01,0.1,1,10,100],\n              'clf__penalty': ['l1', 'l2']}\ngrid = GridSearchCV(pipe, param_grid, return_train_score=False,\n                    n_jobs=-1, scoring=\"roc_auc\", cv=skf)\nlogreg=grid.fit(X_train, y_train)\ngrid.score(X_test, y_test)\n# View best hyperparameters\nprediction7= grid.predict(X_test)\nconfusion= metrics.confusion_matrix(y_test, prediction7)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\nprint (\"SMOTE roc auc score: \", \"\\n\", metrics.roc_auc_score(y_test,prediction7))\nprint (\"--\"*30)\nprint (\"SMOTE matrix confusion: \", \"\\n\",metrics.confusion_matrix(y_test,prediction7))\nprint (\"--\"*30)\nprint (\"SMOTE sensitivity: \", TP / float(FN + TP))\nprint (\"--\"*30)\nprint (\"SMOTE model specificity: \", TN / (TN + FP))\nprint (\"__\"*30)\nprint (\"SMOTE model accuracy: \", metrics.accuracy_score(y_test,prediction7))\nprint (\"--\"*30)\n\nprint('Best Penalty:', grid.best_estimator_.get_params()['clf__penalty'])\nprint('Best C:', grid.best_estimator_.get_params()['clf__C'])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0287bbd31d417821fd61031768c9c9a5ac8159f7"},"cell_type":"markdown","source":"And a huge incremente in sensitivity and Roc."},{"metadata":{"_uuid":"9bc9f51577830a3f1eda0d5b73e9d1a1611e4dd5"},"cell_type":"markdown","source":"### Linear Discriminant analysis Classifier oversampling with SMOTE"},{"metadata":{"trusted":true,"_uuid":"25b2e40dd2465ddf0327a1238adb44cbc4ca6086"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=444, stratify=y)\n\npipe2 = Pipeline([\n    ('oversample', SMOTE(random_state=444)),\n    ('clf', LinearDiscriminantAnalysis())])\n\nskf2 = StratifiedKFold(n_splits=10)\nparam_grid = {'clf__n_components': [1]}\ngrid = GridSearchCV(pipe2, param_grid, return_train_score=False,\n                    n_jobs=-1, scoring=\"roc_auc\", cv=skf2)\nLDA=grid.fit(X_train, y_train)\ngrid.score(X_test, y_test)\n# View best hyperparameters\nprediction8= LDA.predict(X_test)\nconfusion2= metrics.confusion_matrix(y_test, prediction8)\nTP = confusion2[1, 1]\nTN = confusion2[0, 0]\nFP = confusion2[0, 1]\nFN = confusion2[1, 0]\n\nprint (\"SMOTE roc auc score: \", \"\\n\", metrics.roc_auc_score(y_test,prediction8))\nprint (\"--\"*30)\nprint (\"SMOTE matrix confusion: \", \"\\n\",metrics.confusion_matrix(y_test,prediction8))\nprint (\"--\"*30)\nprint (\"SMOTE sensitivity: \", TP / float(FN + TP))\nprint (\"--\"*30)\nprint (\"SMOTE model specificity: \", TN / (TN + FP))\nprint (\"__\"*30)\nprint (\"SMOTE model accuracy: \", metrics.accuracy_score(y_test,prediction8))\nprint (\"--\"*30)\n\nprint('Best Number of components:', grid.best_estimator_.get_params()['clf__n_components'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"314c00cc044b5f22ab05caf459ddef4a03ff0a08"},"cell_type":"markdown","source":"Also, a huge increment. But in hear the sensitivity is not as good as in the Logistic Regression, so I will keep working with that algorithm."},{"metadata":{"_uuid":"89f05039f2629860fc2bb6a12640b09bfbde0ad0"},"cell_type":"markdown","source":"# Sensitivity Threshold and ROC curve"},{"metadata":{"_uuid":"20ce671c6dbd73197b6dc7651963077e94a03f9c"},"cell_type":"markdown","source":"We have seen now, how the roc auc is growing. But we have a little problem. \nThe specificity is going down. Is this a bad thing?\nWell no. In this case is not, because here we are interested in the sensitivity.\n\n* A high specificity tell us that the correct things are going to be well classified. This is good.\n* A high sensitivity tell us that we are not going to allow any person with diabetes to be classified as a healthy person.\n* In a medical work, is more important to not allow mistakes, rather than making good predictions. So, the good thing now is to improve the sensitivity.\n* Now, I will analyze the sensitivity threshold and see how everything changes. I will be working with the LogisticRegression of the SMOTE technique."},{"metadata":{"_uuid":"d5455c555dc531d9b8311516de0777d56dc109e3"},"cell_type":"markdown","source":"Lets start seeing the first 10 predicted responses."},{"metadata":{"trusted":true,"_uuid":"b264fb074e616c3c7653cbcb283439009cb6aa87"},"cell_type":"code","source":"logreg.predict(X_test)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd96a4e8fce01cc14d9c5438055808b825feff6a"},"cell_type":"markdown","source":"Now, lets see the 10 predicted probabilites for each label. Outcome diabetes and Outcome not diabetes."},{"metadata":{"trusted":true,"_uuid":"bc29992ca7f56717de5bc69387b4190b8b66153e"},"cell_type":"code","source":"logreg.predict_proba(X_test)[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"257e98918d59790cd62157bb5724029f502b3a3f"},"cell_type":"markdown","source":"Each row, numbers sum to 1 (total probability), and we have two columns:\n\n* column 0: predicted probability that each observation is a member of the non diabetic group\n* column 1: predicted probability that each observation is a member of the diabetic group.\n    * Now with that information we can now work with another probability. We can choose the class with the highest probability with the predict proba process. And with that, we can see the classification threshold. Lets continue with the code to understand it better.\n"},{"metadata":{"_uuid":"593e465096f1a0b055c0ad6413787c53798bd0cc"},"cell_type":"markdown","source":"Lets see the first 10 predicted probabilities for the diabetic group."},{"metadata":{"trusted":true,"_uuid":"aed924faea66e85af5c5cdbe8367527c5569a3d8"},"cell_type":"code","source":"logreg.predict_proba(X_test)[0:10, 1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fde2888548b8353406afd52553f2022059ee7b7a"},"cell_type":"markdown","source":"So, the first one has 33% posibilities to be a diabetic person. The same with the others.\nNow, lets save that data to plot it."},{"metadata":{"trusted":true,"_uuid":"06a86c9b38140efbd332e66714b242e080beee26"},"cell_type":"code","source":"y_pred_prob = logreg.predict_proba(X_test)[:, 1] # Here I saved all the samples, not just 10.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fad42b8a2acb0e4b25674746cc9118a7d07d8acf"},"cell_type":"code","source":"plt.hist(y_pred_prob, bins=8)\n\n# x-axis limit from 0 to 1\nplt.xlim(0,1)\nplt.title('Histogram of predicted probabilities')\nplt.xlabel('Predicted probability of diabetes')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7053114ae5b047c62fe7079eacde01d4eeca4067"},"cell_type":"markdown","source":"We see that around the 3-4 bar we have almost all the predictions. This is because the threshold is by default around 0.5. If we change it, we can change also the specificity and sensitivity of our model. And in this case we want to do it to increase the sensitivity.\n\nLets see how to change it."},{"metadata":{"trusted":true,"_uuid":"807e78b368cc4ebd2d6a00bbe41e82a130f4dc1f"},"cell_type":"code","source":"from sklearn.preprocessing import binarize\ny_pred_prob=y_pred_prob.reshape(1,-1)\n# it will return 1 for all values above 0.3 and 0 otherwise\n# results are 2D so we slice out the first column\ny_pred_class = binarize(y_pred_prob, 0.3)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04912a07dd3ac377c7bcbb58a6df715eb6d7f762"},"cell_type":"code","source":"y_pred_prob[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73450a4d11b6ceb08b39efbcca0053a6223898b4"},"cell_type":"code","source":"y_pred_class[0:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0199e30626516cba2f1f8ba0bcb4468cbd0fa640"},"cell_type":"markdown","source":"And this way you can change the sensitivity threshold. Anyways, continue with the code to see how to do it. But in essence, I just need to change the value of binarize to other threshold, and the sensitivity and specificity will change."},{"metadata":{"_uuid":"97325d23f0237ae5f72211c90286bab80c3610a4"},"cell_type":"markdown","source":"# ROC curve"},{"metadata":{"trusted":true,"_uuid":"c91597d442fd5e2283e220697314c222cde09442"},"cell_type":"code","source":"y_pred_prob = logreg.predict_proba(X_test)[:, 1] # remember that equation\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39d9a96e53d5b52de668fa6d82765d26a6297a63"},"cell_type":"markdown","source":"ROC curve can help you  choose a threshold that balances sensitivity and specificity in a way that makes sense for the  context. We cannot see the threshold but we can compute them. \n* Anyway, we need to find a balance between sensitivity and specifity now.\n"},{"metadata":{"trusted":true,"_uuid":"013fdefbda7a166175ece9e0f57b1673221d26aa"},"cell_type":"code","source":"def evaluate_threshold(threshold):\n    print('Sensitivity:', tpr[thresholds > threshold][-1])\n    print('Specificity:', 1 - fpr[thresholds > threshold][-1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71d19fde26ebe5bb1dbb5d7bae43db2ace6c26c6"},"cell_type":"markdown","source":"Now we can see how the values of sensitivity and specifity choosing the threshold. When we find one that we like, we can change the value of the binarize and get an extremely optimized model."},{"metadata":{"trusted":true,"_uuid":"d6e2c4593a240c04a91729be154bec44a7b9c59b"},"cell_type":"code","source":"evaluate_threshold(0.5) # For example.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2032383d940e7be2931a3303980f1478b491d522"},"cell_type":"markdown","source":"Now, lets compute the AUC and finish the work of our model.\n*AUC is the percentage of the ROC plot that is underneath the curve:"},{"metadata":{"trusted":true,"_uuid":"c8abc3e802cda8c56e188ee61390c28422805ac5"},"cell_type":"code","source":"print(metrics.roc_auc_score(y_test, y_pred_prob))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f0afc60fe03e8bc3abf63dfb3d6ff6bedf6c7bd"},"cell_type":"code","source":"from sklearn.cross_validation import cross_val_score\ncross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75cbfe23f21cfde1c78d26fb4a3ff94205adf187"},"cell_type":"markdown","source":"And we have finish. We have a model with a 0.836 % in roc_auc metric, so it is extremely well balanced for this dataset. Also, we can change our threshold and get more specificity or sensitivity whenever we want.\n\nAnd that is all."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}